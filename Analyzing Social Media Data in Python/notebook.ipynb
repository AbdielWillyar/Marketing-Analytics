{"cells":[{"source":"# Collecting data through the Twitter API","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":"Firstly, we need to authenticate ourselves with the Twitter API using Tweepy. This authentication involves using the consumer key and access token, which are required to access the Twitter API. This is the initial step that needs to be taken before we can collect data.","metadata":{},"cell_type":"markdown","id":"6ccab710-05fd-4e34-a423-69132276b5cf"},{"source":"from tweepy import OAuthHandler\nfrom tweepy import API\n\n# Consumer key authentication\nauth = OAuthHandler(consumer_key, consumer_secret)\n\n# Access key authentication\nauth.set_access_token(access_token, access_token_secret)\n\n# Set up the API with the authentication handler\napi = API(auth)","metadata":{},"cell_type":"code","id":"6a02d1ab-05e7-434d-b107-7519d71232b9","execution_count":null,"outputs":[]},{"source":"After successful authentication, we are ready to collect data from Twitter based on specific keywords. We use Tweepy's Stream class to do this. In this step, we specify the keywords we want to monitor and start collecting data that matches these keywords.","metadata":{},"cell_type":"markdown","id":"c196d373-9745-44e5-b0dd-3b2418374be4"},{"source":"from tweepy import Stream\n\n# Set up words to track\nkeywords_to_track = [\"#rstats\", \"#python\"]\n\n# Instantiate the SListener object \nlisten = SListener(api)\n\n# Instantiate the Stream object\nstream = Stream(auth, listen)\n\n# Begin collecting data\nstream.filter(track = keywords_to_track)","metadata":{},"cell_type":"code","id":"b332c2fe-9604-4cbc-b1ce-6205af223821","execution_count":null,"outputs":[]},{"source":"With these steps, we have successfully set up authentication and initiated the collection of Twitter data based on specific keywords. This allows us to gather data relevant to our interests or projects.","metadata":{},"cell_type":"markdown","id":"a1ae2f40-e2d2-4498-aa86-42a4ff071c69"},{"source":"In this code section, we load JSON-formatted tweet data, convert it into a Python object, and then access various aspects of the tweet. We print the tweet's text content and unique ID. Additionally, we access user-related information, including the user's handle, follower count, location, and description. Furthermore, we delve into retweet data by printing the retweeted tweet's text, the text of the original tweet that was retweeted, the user who performed the retweet, and the user who originally posted the tweet being retweeted.","metadata":{},"cell_type":"markdown","id":"fbcfb345-5e37-4088-9bda-cc5ddcca6f12"},{"source":"# Load JSON\nimport json\n\n# Convert from JSON to Python object\ntweet = json.loads(tweet_json)\n\n# Print tweet text\nprint(tweet['text'])\n\n# Print tweet id\nprint(tweet['id'])\n\n# Print user handle\nprint(tweet['user']['screen_name'])\n\n# Print user follower count\nprint(tweet['user']['followers_count'])\n\n# Print user location\nprint(tweet['user']['location'])\n\n# Print user description\nprint(tweet['user']['description'])\n\n# Print the text of the tweet\nprint(rt['text'])\n\n# Print the text of tweet which has been retweeted\nprint(rt['retweeted_status']['text'])\n\n# Print the user handle of the tweet\nprint(rt['user']['screen_name'])\n\n# Print the user handle of the tweet which has been retweeted\nprint(rt['retweeted_status']['user']['screen_name'])","metadata":{},"cell_type":"code","id":"6d47addb-8f85-42a7-bdb4-23c31f97382a","execution_count":null,"outputs":[]},{"source":"# Processing Twitter Text","metadata":{},"cell_type":"markdown","id":"d251942a-9053-4d37-996c-d3f2164fb853"},{"source":"**Tweet Items and Tweet Flattening**\n\nIn the realm of Twitter data analysis, tweets often come with various fields in the Twitter JSON that contain textual data. In a typical tweet, you can find the tweet text, the user's description, and their location. However, there are additional complexities to consider, such as extended tweets for messages longer than 140 characters and quoted tweets, which include both the original tweet's text and the commentary.","metadata":{},"cell_type":"markdown","id":"1fd92107-fca5-4184-b3f4-4e60c67a9050"},{"source":"# Print the tweet text\nprint(quoted_tweet['text'])\n\n# Print the quoted tweet text\nprint(quoted_tweet['quoted_status']['text'])\n\n# Print the quoted tweet's extended (140+) text\nprint(quoted_tweet['quoted_status']['extended_tweet']['full_text'])\n\n# Print the quoted user location\nprint(quoted_tweet['quoted_status']['user']['location'])","metadata":{},"cell_type":"code","id":"f8e4ef24-36b5-40f3-9ce2-ffa26ea34157","execution_count":null,"outputs":[]},{"source":"**A Tweet Flattening Function**\n\nIn Twitter analysis, we often deal with hundreds or thousands of tweets. To streamline the process, we can create a function called flatten_tweets() to flatten JSON data containing tweets. We'll use this function frequently, adjusting it as needed for different data types.","metadata":{},"cell_type":"markdown","id":"f49d594e-37d8-44a4-97f2-0db6325060a8"},{"source":"def flatten_tweets(tweets_json):\n    \"\"\" Flattens out tweet dictionaries so relevant JSON\n        is in a top-level dictionary.\"\"\"\n    tweets_list = []\n    \n    # Iterate through each tweet\n    for tweet in tweets_json:\n        tweet_obj = json.loads(tweet)\n    \n        # Store the user screen name in 'user-screen_name'\n        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']\n    \n        # Check if this is a 140+ character tweet\n        if 'extended_tweet' in tweet_obj:\n            # Store the extended tweet text in 'extended_tweet-full_text'\n            tweet_obj['extended_tweet-full_text'] = tweet_obj['extended_tweet']['full_text']\n    \n        if 'retweeted_status' in tweet_obj:\n            # Store the retweet user screen name in 'retweeted_status-user-screen_name'\n            tweet_obj['retweeted_status-user-screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']\n\n            # Store the retweet text in 'retweeted_status-text'\n            tweet_obj['retweeted_status-text'] = tweet_obj['retweeted_status']['text']\n            \n        tweets_list.append(tweet_obj)\n    return tweets_list","metadata":{},"cell_type":"code","id":"af6617ac-0c07-44f3-ac3b-787df26e5a90","execution_count":null,"outputs":[]},{"source":"**Loading Tweets into a DataFrame**\n\nNow, let's import this processed data into a pandas DataFrame for scalable tweet analysis. We'll be working with a dataset containing tweets that include either the '#rstats' or '#python' hashtag, stored as a list of tweet JSON objects in data_science_json.","metadata":{},"cell_type":"markdown","id":"5511cc99-013f-4e52-8383-c137f01fa51b"},{"source":"# Import pandas\nimport pandas as pd\n\n# Flatten the tweets and store in `tweets`\ntweets = flatten_tweets(data_science_json)\n\n# Create a DataFrame from `tweets`\nds_tweets = pd.DataFrame(tweets)\n\n# Print out the first 5 tweets from this dataset\nprint(ds_tweets['text'].values[0:5])","metadata":{},"cell_type":"code","id":"8d2245ca-93bd-4e24-8565-c2d7a3697017","execution_count":null,"outputs":[]},{"source":"## Counting Word","metadata":{},"cell_type":"markdown","id":"ccc25527-7e2e-47ea-9d57-894def7c492e"},{"source":"**Finding Keywords**\n\nCounting known keywords is a fundamental step in text data analysis, especially when dealing with Twitter datasets. In this dataset, we aim to count the occurrences of specific hashtags within a collection of tweets related to data science. To achieve this, we'll leverage string methods available in the pandas Series object.","metadata":{},"cell_type":"markdown","id":"6020c930-8a3f-475e-8829-bc161b645e07"},{"source":"# Flatten the tweets and store them\nflat_tweets = flatten_tweets(data_science_json)\n\n# Convert to DataFrame\nds_tweets = pd.DataFrame(flat_tweets)\n\n# Find mentions of #python in 'text'\npython = ds_tweets['text'].str.contains('#python', case=False)\n\n# Print the proportion of tweets mentioning #python\nprint(\"Proportion of #python tweets:\", np.sum(python) / ds_tweets.shape[0])","metadata":{},"cell_type":"code","id":"f2315bf2-b3c1-414f-b957-78f172ac645a","execution_count":null,"outputs":[]},{"source":"**Looking for Text in All the Wrong Places**\n\nIt's important to remember that relevant text may not always reside in the main text field of a tweet. It can also be found in the extended_tweet, retweeted_status, or quoted_status. Therefore, we need to check all of these fields to ensure we account for all relevant text. To streamline this process, we'll create a function.\n\nThe function check_word_in_tweet checks if a word is present in various fields of a Twitter dataset, including the main text, extended tweets (for tweets longer than 140 characters), retweets, and quoted tweets. It returns a logical pandas Series.","metadata":{},"cell_type":"markdown","id":"893710d9-d13c-429b-9c49-c0b619ab9e4a"},{"source":"def check_word_in_tweet(word, data):\n    \"\"\"Checks if a word is in a Twitter dataset's text. \n    Checks text and extended tweet (140+ character tweets) for tweets,\n    retweets and quoted tweets.\n    Returns a logical pandas Series.\n    \"\"\"\n    contains_column = data['text'].str.contains(word, case=False)\n    contains_column |= data['extended_tweet-full_text'].str.contains(word, case=False)\n    contains_column |= data['quoted_status-text'].str.contains(word, case=False)\n    contains_column |= data['quoted_status-extended_tweet-full_text'].str.contains(word, case=False)\n    contains_column |= data['retweeted_status-text'].str.contains(word, case=False)\n    contains_column |= data['retweeted_status-extended_tweet-full_text'].str.contains(word, case=False)\n    \n    return contains_column","metadata":{},"cell_type":"code","id":"040c85c0-1df8-404f-a406-8e286e8eb3d7","execution_count":null,"outputs":[]},{"source":"**Comparing #python to #rstats**\n\nWith our versatile function to check for word occurrences in various tweet fields, we can now apply it to multiple words and make comparisons. Returning to our example with the data science hashtag dataset, we want to measure how frequently #rstats appears compared to #python.","metadata":{},"cell_type":"markdown","id":"15104141-27ae-43a3-b7c5-a8b2af0a4eb5"},{"source":"# Find mentions of #python in all text fields\npython = check_word_in_tweet(\"#python\", ds_tweets)\n\n# Find mentions of #rstats in all text fields\nrstats = check_word_in_tweet(\"#rstats\", ds_tweets)\n\n# Print the proportion of tweets mentioning #python\nprint(\"Proportion of #python tweets:\", np.sum(python) / ds_tweets.shape[0])\n\n# Print the proportion of tweets mentioning #rstats\nprint(\"Proportion of #rstats tweets:\", np.sum(rstats) / ds_tweets.shape[0])","metadata":{},"cell_type":"code","id":"f77c55e2-10e8-4ab0-a3ad-bd4321ccbe66","execution_count":null,"outputs":[]},{"source":"# Time Series Analysis","metadata":{},"cell_type":"markdown","id":"8b543670-8df9-420a-99ff-afecec229748"},{"source":"**Creating a Time Series Data Frame**\n\nTime series data is invaluable for tracking variations over time, a valuable approach when analyzing Twitter text data to monitor the prevalence of specific words or phrases. The first step in achieving this is to convert the DataFrame into a format suitable for pandas time series methods. This can be accomplished by converting the index into a datetime type.","metadata":{},"cell_type":"markdown","id":"c027b888-1c2d-4b35-b38e-a5c4ec607f38"},{"source":"# Print 'created_at' to see the original format of datetime in Twitter data\nprint(ds_tweets['created_at'].head())\n\n# Convert the 'created_at' column to np.datetime object\nds_tweets['created_at'] = pd.to_datetime(ds_tweets['created_at'])\n\n# Print 'created_at' to see the new format\nprint(ds_tweets['created_at'].head())\n\n# Set the index of ds_tweets to 'created_at'\nds_tweets = ds_tweets.set_index('created_at')","metadata":{},"cell_type":"code","id":"bd81ebbe-5808-4c71-8b99-f42d82c1cb83","execution_count":null,"outputs":[]},{"source":"**Generating Mean Frequency**\n\nTo analyze and visualize word prevalence over time, we need to create a metric that can be graphed. Our check_word_in_tweet() function returns a boolean Series, where True is equivalent to 1. We can utilize this to produce columns for each keyword of interest and understand their prevalence over time.","metadata":{},"cell_type":"markdown","id":"aa54f131-2f60-4f12-8137-12214c000e7b"},{"source":"# Create a 'python' column\nds_tweets['python'] = check_word_in_tweet('#python', ds_tweets)\n\n# Create an 'rstats' column\nds_tweets['rstats'] = check_word_in_tweet('#rstats', ds_tweets)","metadata":{},"cell_type":"code","id":"26fe65cf-163a-43cd-9daf-b3b556bd44c8","execution_count":null,"outputs":[]},{"source":"**Plotting Mean Frequency**\n\nFinally, we'll create a daily average of hashtag mentions and plot them over time. We'll calculate the proportions from the two boolean Series on a daily basis and then visualize them.","metadata":{},"cell_type":"markdown","id":"6d674e06-6915-48e4-80d5-fc0425061d99"},{"source":"# Average of 'python' column by day\nmean_python = ds_tweets['python'].resample('1 d').mean()\n\n# Average of 'rstats' column by day\nmean_rstats = ds_tweets['rstats'].resample('1 d').mean()\n\n# Plot mean 'python' by day (green) and mean 'rstats' by day (blue)\nplt.plot(mean_python.index.day, mean_python, color='green')\nplt.plot(mean_rstats.index.day, mean_rstats, color='blue')\n\n# Add labels and show\nplt.xlabel('Day')\nplt.ylabel('Frequency')\nplt.title('Language Mentions Over Time')\nplt.legend(('#python', '#rstats'))\nplt.show()","metadata":{},"cell_type":"code","id":"a9379025-cbe0-4e7f-8fd2-ffc0f0ab8668","execution_count":null,"outputs":[]},{"source":"![image](meanfreq.png)\n","metadata":{},"cell_type":"markdown","id":"da5f197c-443d-4197-bb02-49437672d120"},{"source":"This time series analysis helps us understand how the popularity of these keywords evolves on Twitter, providing insights into trends and user interests.","metadata":{},"cell_type":"markdown","id":"fd0789fb-5c13-401f-ba37-25db7d9730c8"},{"source":"# Sentiment Analysis","metadata":{},"cell_type":"markdown","id":"242d0614-dc80-4fec-afa3-5cfb80d91d33"},{"source":"**Loading VADER**\n\nSentiment analysis offers us a direct and interpretable method to grasp the meaning behind text data. While it has its limitations, it serves as an excellent starting point when working with textual data. In Python, several out-of-the-box tools are available for sentiment analysis.","metadata":{},"cell_type":"markdown","id":"6bb536fc-c836-403c-8291-80f80684e3c2"},{"source":"# Load SentimentIntensityAnalyzer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \n\n# Instantiate a new SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# Generate sentiment scores\nsentiment_scores = ds_tweets['text'].apply(sid.polarity_scores)","metadata":{},"cell_type":"code","id":"57e1a5a3-5108-4954-94b4-1e9d71218cba","execution_count":null,"outputs":[]},{"source":"**Calculating Sentiment Scores**\n\nTo gain a rough measure of sentiment towards a specific hashtag, we can calculate the average sentiment for tweets mentioning that hashtag. However, it's essential to remember that a tweet can encompass various elements, so it's crucial to inspect both the tweet's text and metrics generated by automated text methods.","metadata":{},"cell_type":"markdown","id":"96190cd1-ae72-46dc-8147-6a379e2af590"},{"source":"# Print out the text of a positive tweet\nprint(ds_tweets[sentiment_scores > .6]['text'].values[0])\n\n# Print out the text of a negative tweet\nprint(ds_tweets[sentiment_scores < -.6]['text'].values[0])\n\n# Generate average sentiment scores for #python\nsentiment_py = sentiment_scores[check_word_in_tweet('#python', ds_tweets)].resample('1 d').mean()\n\n# Generate average sentiment scores for #rstats\nsentiment_r = sentiment_scores[check_word_in_tweet('#rstats', ds_tweets)].resample('1 d').mean()","metadata":{},"cell_type":"code","id":"4619e13d-2db7-4e9e-b677-3241596a1080","execution_count":null,"outputs":[]},{"source":"**Plotting Sentiment Scores**\n\nFinally, let's visualize the sentiment of each hashtag over time. This process is quite similar to plotting tweet prevalence.","metadata":{},"cell_type":"markdown","id":"71198fe6-d425-4310-ab0e-9806cf2fda88"},{"source":"# Import matplotlib\nimport matplotlib.pyplot as plt\n\n# Plot average #python sentiment per day\nplt.plot(sentiment_py.index.day, sentiment_py, color='green')\n\n# Plot average #rstats sentiment per day\nplt.plot(sentiment_r.index.day, sentiment_r, color='blue')\n\nplt.xlabel('Day')\nplt.ylabel('Sentiment')\nplt.title('Sentiment of Data Science Languages')\nplt.legend(('#python', '#rstats'))\nplt.show()","metadata":{},"cell_type":"code","id":"5c4cfba7-6c64-4ede-b25a-eff22a8637ba","execution_count":null,"outputs":[]},{"source":"![image](sentscore.png)\n","metadata":{},"cell_type":"markdown","id":"632932d1-71a8-451d-aaf2-5a0b1f6ab42f"},{"source":"This sentiment analysis provides valuable information about the emotional tone associated with these hashtags on Twitter, helping us gauge public sentiment towards specific topics.","metadata":{},"cell_type":"markdown","id":"75feac54-158f-4cfa-bf7f-4ad447f7c9d8"},{"source":"# Twitter Networks","metadata":{},"cell_type":"markdown","id":"9859856f-387a-429b-a1a7-1ac1a2683caf"},{"source":"**Creating a Retweet Network**\n\nTwitter data inherently exhibits networked characteristics. Among the essential Twitter networks are retweet networks, often represented as directed graphs, where the retweeting user is the source and the retweeted user is the target. By leveraging Twitter data within our flattened DataFrame, we can import this data into networkx and construct a retweet network.","metadata":{},"cell_type":"markdown","id":"d51c9618-49d3-46bb-a293-08e701dd856d"},{"source":"# Import networkx\nimport networkx as nx\n\n# Create a retweet network from the edgelist\nG_rt = nx.from_pandas_edgelist(\n    sotu_retweets,\n    source='user-screen_name',\n    target='retweeted_status-user-screen_name',\n    create_using=nx.DiGraph())\n\n# Print the number of nodes\nprint('Nodes in RT network:', len(G_rt.nodes()))\n\n# Print the number of edges\nprint('Edges in RT network:', len(G_rt.edges()))","metadata":{},"cell_type":"code","id":"0be3844a-f792-4254-816d-f622c2d21311","execution_count":null,"outputs":[]},{"source":"**Creating a Reply Network**\n\nReply networks exhibit a distinct structure compared to retweet networks. While retweet networks often indicate agreement, reply networks signal discussion, deliberation, and disagreement. The essential network properties, such as directionality and source-target relationships, remain consistent.","metadata":{},"cell_type":"markdown","id":"8d7ee45e-9cc3-4ac8-a222-69184aca1f8f"},{"source":"# Import networkx\nimport networkx as nx\n\n# Create a reply network from the edgelist\nG_reply = nx.from_pandas_edgelist(\n    sotu_replies,\n    source='user-screen_name',\n    target='in_reply_to_screen_name',\n    create_using=nx.DiGraph())\n\n# Print the number of nodes\nprint('Nodes in reply network:', len(G_reply.nodes()))\n\n# Print the number of edges\nprint('Edges in reply network:', len(G_reply.edges()))","metadata":{},"cell_type":"code","id":"f8dafc61-beb0-44e8-ad09-162353e31c18","execution_count":null,"outputs":[]},{"source":"**Visualizing the Retweet Network**\n\nVisualizing retweet networks is a crucial step in exploratory data analysis. It allows us to inspect the network's structure visually, identify users with significant influence, and discern various spheres of conversation.","metadata":{},"cell_type":"markdown","id":"d3a15869-c4fb-4005-8201-dd9a16d08090"},{"source":"# Create random layout positions\npos = nx.random_layout(G_rt)\n\n# Create a size list\nsizes = [x[1] for x in G_rt.degree()]\n\n# Draw the network\nnx.draw_networkx(G_rt, pos,\n    with_labels=False,\n    node_size=sizes,\n    width=0.1, alpha=0.7,\n    arrowsize=2, linewidths=0)\n\n# Turn the axis off and show\nplt.axis('off')\nplt.show()","metadata":{},"cell_type":"code","id":"c8eda808-87a2-454e-950d-1c5c48699fdb","execution_count":null,"outputs":[]},{"source":"![image](retweetnet.png)\n","metadata":{},"cell_type":"markdown","id":"55bd7fe5-ecf3-4cd2-ab37-a558d002aed4"},{"source":"**In-Degree Centrality**\n\nCentrality measures the importance of a node in a network. Degree centrality, especially in retweet networks, is a straightforward and intuitively understandable measure. However, in directed networks like Twitter, we need to differentiate between in-degree and out-degree centrality. In-degree centrality in retweet networks highlights users who receive many retweets.","metadata":{},"cell_type":"markdown","id":"593b363b-b436-4b37-a4d5-08733a908412"},{"source":"# Generate in-degree centrality for retweets\nrt_centrality = nx.in_degree_centrality(G_rt)\n\n# Generate in-degree centrality for replies\nreply_centrality = nx.in_degree_centrality(G_reply)\n\n# Store centralities in DataFrames\nrt = pd.DataFrame(list(rt_centrality.items()), columns=column_names)\nreply = pd.DataFrame(list(reply_centrality.items()), columns=column_names)\n\n# Print the first five results in descending order of centrality\nprint(rt.sort_values('degree_centrality', ascending=False).head())\n\n# Print the first five results in descending order of centrality\nprint(reply.sort_values('degree_centrality', ascending=False).head())","metadata":{},"cell_type":"code","id":"0b968f8a-0634-4e61-9849-b148df970778","execution_count":null,"outputs":[]},{"source":"**Betweenness Centrality**\n\nBetweenness centrality in retweet and reply networks identifies users who bridge different Twitter communities. These communities may be linked by topic or ideology.","metadata":{},"cell_type":"markdown","id":"2e371574-5064-48d3-8356-d29038382c8b"},{"source":"# Generate betweenness centrality for retweets\nrt_centrality = nx.betweenness_centrality(G_rt)\n\n# Generate betweenness centrality for replies\nreply_centrality = nx.betweenness_centrality(G_reply)\n\n# Store centralities in DataFrames\nrt = pd.DataFrame(rt_centrality.items(), columns=column_names)\nreply = pd.DataFrame(reply_centrality.items(), columns=column_names)\n\n# Print the first five results in descending order of centrality\nprint(rt.sort_values('betweenness_centrality', ascending=False).head())\n\n# Print the first five results in descending order of centrality\nprint(reply.sort_values('betweenness_centrality', ascending=False).head())","metadata":{},"cell_type":"code","id":"429ede5a-7489-40a1-83ce-a89c95499cb3","execution_count":null,"outputs":[]},{"source":"**Ratios: \"The Ratio\"**\n\nWhile not a strict measure of network importance, \"The Ratio\" is a Twitter-specific network measure often used to assess a tweet's unpopularity. It's calculated by dividing the number of replies by the number of retweets. In our case, we focus on the in-degrees of both retweet and reply networks.","metadata":{},"cell_type":"markdown","id":"f94ce22d-dd49-4d0b-8e4e-3472d7c1ae45"},{"source":"# Calculate in-degrees and store in DataFrames\ndegree_rt = pd.DataFrame(list(G_rt.in_degree()), columns=column_names)\ndegree_reply = pd.DataFrame(list(G_reply.in_degree()), columns=column_names)\n\n# Merge the two DataFrames on screen name\nratio = degree_rt.merge(degree_reply, on='screen_name', suffixes=('_rt', '_reply'))\n\n# Calculate the ratio\nratio['ratio'] = ratio['degree_reply'] / ratio['degree_rt']\n\n# Exclude any tweets with fewer than 5 retweets\nratio = ratio[ratio['degree_rt'] >= 5]\n\n# Print the first five with the highest ratio\nprint(ratio.sort_values('ratio', ascending=False).head())","metadata":{},"cell_type":"code","id":"97781211-2b9e-4113-b2a9-fb6213f7a214","execution_count":null,"outputs":[]},{"source":"Additional Result:","metadata":{},"cell_type":"markdown","id":"1ccba7fd-2913-4889-bc03-2b045841d95a"},{"source":"\tscreen_name  degree_rt  degree_reply  ratio\n\tSpeakerRyan      8           15  \t  1.875\n\tNBCNews         20           18  \t  0.900\n\tbenshapiro       5            4  \t  0.800\n\tSenateGOP        5            3  \t  0.600\n\tCBSThisMorning   6            3  \t  0.500","metadata":{},"cell_type":"markdown","id":"5106d38b-dae0-445a-8d52-1222cdd48de2"},{"source":"**Additional Result Insight:**\n\n- Among the users analyzed, **SpeakerRyan** stands out with the highest ratio of **1.875**. This suggests that **SpeakerRyan's** tweets received a significantly higher number of replies compared to retweets during the event.\n\n- **NBCNews** and **benshapiro** also have notable ratios, indicating substantial engagement and discussion on their tweets.\n\n- **SenateGOP** and **CBSThisMorning** also have interesting ratios, reflecting varying levels of engagement and conversation around their tweets.","metadata":{},"cell_type":"markdown","id":"3ede8577-12ef-47ba-83cd-394d0c1b6dc3"},{"source":"Overall, this Twitter network analysis provides valuable insights into user engagement, conversation dynamics, and influential figures during the 2018 State of the Union speech event on Twitter. These insights can be further explored and leveraged for various research or decision-making purposes.","metadata":{},"cell_type":"markdown","id":"68fe4148-b2b7-412e-9e3a-12ce5be97393"},{"source":"# Maps and Twitter data","metadata":{},"cell_type":"markdown","id":"ff2f98c2-5e32-40b2-b701-a4ee9e214248"},{"source":"**Accessing User-Defined Location:**\n\nTo access user-defined locations in tweets, we can extract the 'user-location' field from the Twitter JSON. This field may contain information provided by users in their profiles, which can be imprecise but more readily available.","metadata":{},"cell_type":"markdown","id":"1d767c32-f9b6-4659-a1b0-3ea3eff90ea8"},{"source":"# Extract the user-defined location from a single example tweet\nprint(tweet_json['user']['location'])\n\n# Flatten and load the SOTU tweets into a dataframe\ntweets_sotu = pd.DataFrame(flatten_tweets(tweets_sotu_json))\n\n# Print out the top five user-defined locations\nprint(tweets_sotu['user-location'].value_counts().head())","metadata":{},"cell_type":"code","id":"70d3e94a-292f-489d-a762-d268d6c71d4c","execution_count":null,"outputs":[]},{"source":"**Accessing Bounding Box:**\n\nTweets with coordinate-level geographical information often come with bounding boxes. These are sets of four longitudinal/latitudinal coordinates that represent specific geographical areas. We can extract bounding box data from the 'place' field in the Twitter JSON.","metadata":{},"cell_type":"markdown","id":"22bbd193-fe1b-40d3-940f-9ac2a11127c8"},{"source":"def getBoundingBox(place):\n    \"\"\" Returns the bounding box coordinates.\"\"\"\n    return place['bounding_box']['coordinates']\n\n# Apply the function to get bounding box coordinates\nbounding_boxes = tweets_sotu['place'].apply(getBoundingBox)\n\n# Print out the first bounding box coordinates\nprint(bounding_boxes.values[0])","metadata":{},"cell_type":"code","id":"737b1508-2448-47c8-986e-a6a4fab12b14","execution_count":null,"outputs":[]},{"source":"**Calculating the Centroid:**\n\nTo simplify the handling of bounding boxes, we can calculate the centroid, which represents the center of the bounding box. The centroid is computed by finding the midpoint of the lines formed by the latitude and longitude coordinates.","metadata":{},"cell_type":"markdown","id":"91eb92f9-9e9b-4d93-a858-af2bed14cb7d"},{"source":"def calculateCentroid(place):\n    \"\"\" Calculates the centroid from a bounding box.\"\"\"\n    # Obtain the coordinates from the bounding box.\n    coordinates = place['bounding_box']['coordinates'][0]\n        \n    longs = np.unique( [x[0] for x in coordinates] )\n    lats  = np.unique( [x[1] for x in coordinates] )\n\n    if len(longs) == 1 and len(lats) == 1:\n        # Return a single coordinate\n        return (longs[0], lats[0])\n    elif len(longs) == 2 and len(lats) == 2:\n        # If we have two longs and lats, we have a box.\n        central_long = np.sum(longs) / 2\n        central_lat  = np.sum(lats) / 2\n    else:\n        raise ValueError(\"Non-rectangular polygon not supported: %s\" % \n            \",\".join(map(lambda x: str(x), coordinates)) )\n\n    return (central_long, central_lat)\n\n# Calculate the centroids of the place field\ncentroids = tweets_sotu['place'].apply(calculateCentroid)","metadata":{},"cell_type":"code","id":"43c31ef3-b120-4e83-941a-f34b33330e79","execution_count":null,"outputs":[]},{"source":"**Creating a Basemap Map:**\n\nThe Basemap library allows us to create maps in Python. We can set up a Basemap object and define a bounding box for the map. The map can be customized with various features such as continents, coastlines, countries, and states.","metadata":{},"cell_type":"markdown","id":"c32f1ec1-bb42-4436-81c9-33346d5b0af3"},{"source":"# Set up the US bounding box\nus_boundingbox = [-125, 22, -64, 50] \n\n# Set up the Basemap object\nm = Basemap(llcrnrlon=us_boundingbox[0],\n            llcrnrlat=us_boundingbox[1],\n            urcrnrlon=us_boundingbox[2],\n            urcrnrlat=us_boundingbox[3],\n            projection='merc')\n\n# Customize the map with features\nm.fillcontinents(color='white')\nm.drawcoastlines(color='gray')\nm.drawcountries(color='gray')\nm.drawstates(color='gray')\n\n# Show the map\nplt.show()","metadata":{},"cell_type":"code","id":"aebcee56-0cf8-4df6-b8f8-6ad9ab1a9563","execution_count":null,"outputs":[]},{"source":"**Plotting Centroid Coordinates:**\n\nOnce we have calculated the centroids, we can plot them on the Basemap map by isolating the longitudes and latitudes and using the .scatter() method.","metadata":{},"cell_type":"markdown","id":"59d9e163-c563-41a9-8ec1-79a4c1694e4b"},{"source":"# Calculate the centroids for the dataset and isolate coordinates\ncentroids = tweets_sotu['place'].apply(calculateCentroid)\nlon = [x[0] for x in centroids]\nlat = [x[1] for x in centroids]\n\n# Customize the map with features\nm.fillcontinents(color='white', zorder=0)\nm.drawcoastlines(color='gray')\nm.drawcountries(color='gray')\nm.drawstates(color='gray')\n\n# Plot the centroids\nm.scatter(lon, lat, latlon=True, alpha=0.7)\n\n# Show the map\nplt.show()","metadata":{},"cell_type":"code","id":"577d949e-c57a-425c-b56a-7bccc3de1fe9","execution_count":null,"outputs":[]},{"source":"**Coloring by Sentiment:**\n\nTo differentiate places based on sentiment, we can use sentiment analysis scores from Chapter 2. We extract the compound sentiment score and use it to color the centroids on the map.","metadata":{},"cell_type":"markdown","id":"c4083e97-53c2-45ba-8ebe-8181820af5b2"},{"source":"# Generate sentiment scores\nsentiment_scores = tweets_sotu['text'].apply(sid.polarity_scores)\n\n# Isolate the compound sentiment score\nsentiment_scores = [x['compound'] for x in sentiment_scores]\n\n# Customize the map with features\nm.fillcontinents(color='white', zorder=0)\nm.drawcoastlines(color='gray')\nm.drawcountries(color='gray')\nm.drawstates(color='gray')\n\n# Color centroids based on sentiment scores\nm.scatter(lon, lat, latlon=True, c=sentiment_scores, cmap='coolwarm', alpha=0.7)\n\n# Show the map\nplt.show()","metadata":{},"cell_type":"code","id":"b382f854-96ec-4d44-8c6b-f63c37c47673","execution_count":null,"outputs":[]},{"source":"![image](image.png)\n","metadata":{},"cell_type":"markdown","id":"5f649f59-83eb-4a52-884e-0ee6eb1e8ccd"},{"source":"# All Insight:","metadata":{},"cell_type":"markdown","id":"63e76ccc-d969-407b-9fd6-be56133b6a67"},{"source":"### Loading and Accessing Tweets:\n\n- The initial part of the portfolio demonstrates loading and accessing Twitter data using Python.\n- Twitter data is commonly stored in JSON format, and Python's JSON library is used to convert it into a Python object.\n- Key information like tweet text, user details, and retweet data can be easily accessed from the JSON structure.\n\n**Counting Words:**\n\n- This section focuses on counting specific keywords, like hashtags, in a collection of tweets about data science.\n- The pandas Series object is employed to count keyword occurrences efficiently.\n- It provides a simple yet powerful way to analyze text data in a Twitter dataset.\n\n### Time Series Analysis:\n\n- Time series data frames are created to analyze the variation of specific keywords or phrases over time.\n- Timestamps in tweets are converted into datetime objects for time-based analysis.\n- Mean frequency of keywords is calculated and plotted over time, enabling the tracking of keyword prevalence.\n\n### Sentiment Analysis:\n\n- Sentiment analysis using the VADER library provides insights into the overall sentiment of tweets.\n- Sentiment scores for each tweet are generated using polarity scores, and average sentiment scores are calculated over time.\n- This helps in understanding how sentiments evolve regarding specific hashtags.\n\n### Twitter Networks:\n\n- Focusing on retweet and reply networks.\n- These networks are represented as directed graphs, where users retweet or reply to others.\n- Metrics like in-degree centrality and betweenness centrality are computed to identify influential users and bridge connectors between communities.\n\n**Ratios and Unpopularity:**\n\n- A unique Twitter measure, \"The Ratio,\" is calculated by dividing the number of replies by the number of retweets.\n- The portfolio identifies tweets with high ratios, which typically indicate unpopularity or controversy.\n- This analysis helps in understanding public sentiment regarding specific tweets.\n\n### Putting Twitter Data on the Map:\n\n- Geospatial analysis is conducted by extracting user-defined locations and bounding boxes.\n- Centroids of bounding boxes are computed to represent locations more precisely.\n- Maps are created using Basemap, and centroids are plotted on the map to visualize tweet distribution.\n- Coloring centroids by sentiment scores provides insights into how the State of the Union speech was received in different areas.","metadata":{},"cell_type":"markdown","id":"63820ad9-6f76-49fa-8e72-d26c94383670"},{"source":"# In conclusion, \n\nThis portfolio demonstrates a comprehensive analysis of Twitter data, covering keyword counting, time series analysis, sentiment analysis, network analysis, geospatial analysis, and visualization. It allows for valuable insights into user behavior, sentiments, and geographical patterns in Twitter data, which can be applied to various research and business contexts.","metadata":{},"cell_type":"markdown","id":"3074696f-0627-481f-b9a8-538f10dae5ac"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}